{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RXA3_zObUkl",
    "outputId": "41533b0a-3871-4ceb-ee93-4f4e9fe6350c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "metric = evaluate.load('seqeval')\n",
    "map = {0: \"O\", 1: \"B-FARMACO\", 2: \"I-FARMACO\"}\n",
    "reverse_map = {'O':0, 'B-FARMACO': 1, 'I-FARMACO': 2}\n",
    "tokenizer_kwargs = {'padding': True, 'truncation':True, 'max_length':512}\n",
    "\n",
    "task = \"ner\"\n",
    "#base_model = 'aaaksenova/xlmr_medical'\n",
    "#output_path = 'output/models'\n",
    "#model_checkpoint = f\"aaaksenova/xlmr_drug_classifier\"\n",
    "num_labels = 3\n",
    "target_label = 'FARMACO'\n",
    "\n",
    "#aaaksenova/xlmr_medical - for just xlmr\n",
    "#aaaksenova/xlmr_drug_classifier + aaaksenova/xlmr_medical for filtering + xlmr\n",
    "#Spanish: aaaksenova/SpanishRoberta_multicardioner , \n",
    "#English: aaaksenova/BioLinkBert_multicardioner , \n",
    "#Italian: aaaksenova/SpanishRoberta_it_medprocner\n",
    "MODEL_CLASS = \"aaaksenova/xlmr_drug_classifier\"\n",
    "MODEL_NER = \"aaaksenova/xlmr_medical\"\n",
    "use_filtering = True\n",
    "test_name = 'test' #dev\n",
    "lang = 'it'\n",
    "\n",
    "model_name = MODEL_NER[MODEL_NER.index('/')+1:]\n",
    "sentences_file = f'data/{test_name}_sentences.tsv'\n",
    "result_file = f'output/multicardioner_track2_{test_name}_{lang}_predictions_{model_name}_{use_filtering}.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/multicardioner_track2_test_it_predictions_xlmr_medical_True.tsv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>batch_number</th>\n",
       "      <th>batch_start</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Setting: primary care (PC).\\n</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>Reason for consultation: 26-year-old woman who...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>She had been seen at the PC emergency centre f...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>4</td>\n",
       "      <td>197</td>\n",
       "      <td>She explained abdominal pain of 2 weeks' evolu...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>5</td>\n",
       "      <td>333</td>\n",
       "      <td>Clinical history: personal history of no inter...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   filename  batch_number  batch_start  \\\n",
       "0  multicardioner_test+bg_1             1            0   \n",
       "1  multicardioner_test+bg_1             2           28   \n",
       "2  multicardioner_test+bg_1             3          132   \n",
       "3  multicardioner_test+bg_1             4          197   \n",
       "4  multicardioner_test+bg_1             5          333   \n",
       "\n",
       "                                                text lang  \n",
       "0                      Setting: primary care (PC).\\n   en  \n",
       "1  Reason for consultation: 26-year-old woman who...   en  \n",
       "2  She had been seen at the PC emergency centre f...   en  \n",
       "3  She explained abdominal pain of 2 weeks' evolu...   en  \n",
       "4  Clinical history: personal history of no inter...   en  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_sentences = pd.read_csv(sentences_file, sep='\\t', keep_default_na=False)\n",
    "df_dev_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKENIZATION_REGEX = re.compile(r\"([0-9\\w]+|[^0-9\\w])\", re.UNICODE) # â€™ - es, ' - en, ' - it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    original_token_offsets = []\n",
    "\n",
    "    offset = 0\n",
    "    new_offset = 0\n",
    "    nonspace_token_seen = False\n",
    "\n",
    "    tokens = [t for t in TOKENIZATION_REGEX.split(text) if t]\n",
    "    for t in tokens:\n",
    "        if not t.isspace():\n",
    "            original_token_offsets.append([offset, offset + len(t), t, new_offset, new_offset + len(t)])\n",
    "            nonspace_token_seen = True\n",
    "            new_offset += len(t) + 1\n",
    "        offset += len(t)\n",
    "        \n",
    "\n",
    "    tokenized_sentence = ' '.join([l[2] for l in original_token_offsets])\n",
    "\n",
    "    # store original token offsets\n",
    "    # pass the tokenized string for prediction\n",
    "    return tokenized_sentence, original_token_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylvia/.local/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task=\"text-classification\", model=MODEL_CLASS, device='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NER)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NER).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 512\n",
    "ner_pipe = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", stride=0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'FARMACO',\n",
       "  'score': 0.9957895,\n",
       "  'word': 'Cefazolin2g',\n",
       "  'start': 0,\n",
       "  'end': 11},\n",
       " {'entity_group': 'FARMACO',\n",
       "  'score': 0.9987437,\n",
       "  'word': 'gentamicin',\n",
       "  'start': 23,\n",
       "  'end': 33},\n",
       " {'entity_group': 'FARMACO',\n",
       "  'score': 0.99871254,\n",
       "  'word': 'rifampicin',\n",
       "  'start': 49,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe('Cefazolin2g c/8 hs iv; gentamicin 3mg/kg/day iv; rifampicin 600 mg c/12 hsvo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RjIURpjUvxLW"
   },
   "outputs": [],
   "source": [
    "# group annotations around a clinical procedure mention, based on the annotation label\n",
    "def group_annotations_strict(annotations):\n",
    "    groups = []\n",
    "    i = 0\n",
    "    while i < len(annotations):\n",
    "        if annotations[i]['entity_group'] == 'LABEL_0':\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        group = [] # for the strict strategy, a group is a B (or many Bs), followed by 1 or more Is\n",
    "        if annotations[i]['entity_group'] == 'LABEL_1':\n",
    "            group.append(annotations[i])\n",
    "            i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_1'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_2'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            groups.append(group)\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SsntbSAryqAn"
   },
   "outputs": [],
   "source": [
    "# merge grouped annotations to form a complete entity mention\n",
    "def merge_annotation_group_entries(annotation_group, sentence):\n",
    "    start = annotation_group[0]['start']\n",
    "    end = annotation_group[len(annotation_group) - 1]['end']\n",
    "    text = sentence[start:end]\n",
    "    return {'start': start, 'end': end, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8f574bI6qEwE"
   },
   "outputs": [],
   "source": [
    "def get_mentions(sentence):\n",
    "    if use_filtering:\n",
    "        out = classifier(sentence, **tokenizer_kwargs)\n",
    "    #print(out[0]['label'] == 'ent')\n",
    "    if not use_filtering or out[0]['label'] == \"ent\":\n",
    "        ner_result = ner_pipe(sentence)\n",
    "        return [{'start': mention['start'], 'end': mention['end'], 'text': mention['word']} for mention in ner_result]\n",
    "        #annotation_groups = group_annotations_strict(ner_result)        \n",
    "        #return [merge_annotation_group_entries(group, sentence) for group in annotation_groups]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_mention_offset(mentions, sentence, original_token_offsets, original_sentence, filename, batch_start):\n",
    "    original_mention_offsets = []\n",
    "    current_offset_idx = 0 \n",
    "    for mention in mentions:\n",
    "        start = mention['start']\n",
    "        end = mention['end']\n",
    "        \n",
    "        original_start = -1\n",
    "        original_end = -1\n",
    "        while current_offset_idx < len(original_token_offsets):\n",
    "            token = original_token_offsets[current_offset_idx]\n",
    "            \n",
    "            if token[3] <= start:\n",
    "                original_start = token[0]\n",
    "            \n",
    "            if token[4] >= end:\n",
    "                original_end = token[1]\n",
    "                break\n",
    "            \n",
    "            current_offset_idx += 1\n",
    "\n",
    "        sentence_no_spaces = sentence[start:end].replace(' ', '')\n",
    "        original_sentence_no_spaces = original_sentence[original_start:original_end].replace(' ', '')\n",
    "        # check whether the detected span is contained in the original\n",
    "        if sentence_no_spaces != original_sentence_no_spaces and not(sentence_no_spaces in original_sentence_no_spaces):\n",
    "            print('potential offset issue ', filename, sentence[start:end], original_sentence[original_start:original_end])\n",
    "        if original_start == -1 or original_end == -1:\n",
    "            print('mention not found ', filename, mention)\n",
    "            \n",
    "        original_mention_offsets.append({\n",
    "            'filename': filename, \n",
    "            'start_span':original_start+batch_start, \n",
    "            'end_span':original_end+batch_start, \n",
    "            'text': original_sentence[original_start:original_end]\n",
    "        })\n",
    "    \n",
    "    return original_mention_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Treatment was started with piperacillin tazobactam 4.5g iv every 8h (after taking blood cultures) for 4 days.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Treatment was started with piperacillin tazobactam 4 . 5g iv every 8h ( after taking blood cultures ) for 4 days .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'FARMACO',\n",
       "  'score': 0.9985007,\n",
       "  'word': 'piperacillin tazobactam',\n",
       "  'start': 27,\n",
       "  'end': 50}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb43SpEHQsI9",
    "outputId": "40405391-c271-4057-ba9f-457c8d2f8528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 27, 'end': 50, 'text': 'piperacillin tazobactam'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = get_mentions(tokenized_sentence)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'filename',\n",
       "  'start_span': 37,\n",
       "  'end_span': 60,\n",
       "  'text': 'piperacillin tazobactam'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_original_mention_offset(mentions, tokenized_sentence, original_token_offsets, text, 'filename', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_spans(doc, mentions):\n",
    "    spans = []\n",
    "    current_offset_idx = 0 \n",
    "    for mention in mentions:\n",
    "        start = mention['start']\n",
    "        end = mention['end']\n",
    "        \n",
    "        span_start = -1\n",
    "        span_end = -1\n",
    "        while current_offset_idx < len(doc):\n",
    "            token = doc[current_offset_idx]\n",
    "            \n",
    "            if token.idx <= start:\n",
    "                span_start = token.idx\n",
    "            \n",
    "            if token.idx + len(token) >= end:\n",
    "                span_end = token.idx + len(token)\n",
    "                break\n",
    "            \n",
    "            current_offset_idx += 1\n",
    "        \n",
    "        spans.append(doc.char_span(span_start, span_end))\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "def get_filtered_mentions(doc, phrase_mentions, mentions):\n",
    "    span_mentions = get_mention_spans(doc, mentions)\n",
    "    #phrase_mentions.extend(span_mentions)\n",
    "    filtered_matches = filter_spans(span_mentions)\n",
    "    return [{'start': doc[match.start].idx,'end': doc[match.end-1].idx + len(doc[match.end-1]), 'text':doc[match.start:match.end]} for match in filtered_matches if len(match) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylvia/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 386500 sentences\n",
      "processed 387000 sentences\n",
      "processed 387500 sentences\n",
      "processed 388000 sentences\n",
      "processed 388500 sentences\n",
      "processed 389000 sentences\n",
      "processed 389500 sentences\n",
      "processed 390000 sentences\n",
      "processed 390500 sentences\n",
      "processed 391000 sentences\n",
      "processed 391500 sentences\n",
      "processed 392000 sentences\n",
      "processed 392500 sentences\n",
      "processed 393000 sentences\n",
      "processed 393500 sentences\n",
      "processed 394000 sentences\n",
      "processed 394500 sentences\n",
      "processed 395000 sentences\n",
      "processed 395500 sentences\n",
      "processed 396000 sentences\n",
      "processed 396500 sentences\n",
      "processed 397000 sentences\n",
      "processed 397500 sentences\n",
      "processed 398000 sentences\n",
      "processed 398500 sentences\n",
      "processed 399000 sentences\n",
      "processed 399500 sentences\n",
      "potential offset issue  multicardioner_test+bg_1573 acido clavulanico - acido clavulanico\n",
      "-\n",
      "processed 400000 sentences\n",
      "processed 400500 sentences\n",
      "processed 401000 sentences\n",
      "processed 401500 sentences\n",
      "processed 402000 sentences\n",
      "processed 402500 sentences\n",
      "processed 403000 sentences\n",
      "processed 403500 sentences\n",
      "processed 404000 sentences\n",
      "processed 404500 sentences\n",
      "processed 405000 sentences\n",
      "processed 405500 sentences\n",
      "processed 406000 sentences\n",
      "processed 406500 sentences\n",
      "processed 407000 sentences\n",
      "processed 407500 sentences\n",
      "processed 408000 sentences\n",
      "processed 408500 sentences\n",
      "potential offset issue  multicardioner_test+bg_1929 Clobetasolo propionato - Clobetasolo propionato Clobetasolo propionato\n",
      "- Clobetasolo propionato\n",
      "processed 409000 sentences\n",
      "processed 409500 sentences\n",
      "processed 410000 sentences\n",
      "processed 410500 sentences\n",
      "processed 411000 sentences\n",
      "processed 411500 sentences\n",
      "processed 412000 sentences\n",
      "processed 412500 sentences\n",
      "processed 413000 sentences\n",
      "processed 413500 sentences\n",
      "processed 414000 sentences\n",
      "processed 414500 sentences\n",
      "processed 415000 sentences\n",
      "processed 415500 sentences\n",
      "processed 416000 sentences\n",
      "processed 416500 sentences\n",
      "processed 417000 sentences\n",
      "processed 417500 sentences\n",
      "processed 418000 sentences\n",
      "processed 418500 sentences\n",
      "processed 419000 sentences\n",
      "processed 419500 sentences\n",
      "processed 420000 sentences\n",
      "processed 420500 sentences\n",
      "processed 421000 sentences\n",
      "processed 421500 sentences\n",
      "potential offset issue  multicardioner_test+bg_2437 Cefuroxima - axetile - Cefuroxima - Trimetoprim / sulfametossazolo - Clindamicina - Eritromicina Cefuroxima-axetile\n",
      "- Cefuroxima\n",
      "- Trimetoprim/sulfametossazolo\n",
      "- Clindamicina\n",
      "- Eritromicina\n",
      "processed 422000 sentences\n",
      "processed 422500 sentences\n",
      "processed 423000 sentences\n",
      "processed 423500 sentences\n",
      "processed 424000 sentences\n",
      "processed 424500 sentences\n",
      "processed 425000 sentences\n",
      "processed 425500 sentences\n",
      "processed 426000 sentences\n",
      "processed 426500 sentences\n",
      "processed 427000 sentences\n",
      "processed 427500 sentences\n",
      "processed 428000 sentences\n",
      "processed 428500 sentences\n",
      "processed 429000 sentences\n",
      "processed 429500 sentences\n",
      "processed 430000 sentences\n",
      "processed 430500 sentences\n",
      "potential offset issue  multicardioner_test+bg_2777 Ampicillina / sulbactam Claritromicina Ampicillina / sulbactam\n",
      "Claritromicina\n",
      "processed 431000 sentences\n",
      "processed 431500 sentences\n",
      "processed 432000 sentences\n",
      "processed 432500 sentences\n",
      "processed 433000 sentences\n",
      "processed 433500 sentences\n",
      "processed 434000 sentences\n",
      "processed 434500 sentences\n",
      "processed 435000 sentences\n",
      "processed 435500 sentences\n",
      "processed 436000 sentences\n",
      "processed 436500 sentences\n",
      "processed 437000 sentences\n",
      "processed 437500 sentences\n",
      "processed 438000 sentences\n",
      "processed 438500 sentences\n",
      "processed 439000 sentences\n",
      "processed 439500 sentences\n",
      "processed 440000 sentences\n",
      "processed 440500 sentences\n",
      "processed 441000 sentences\n",
      "processed 441500 sentences\n",
      "processed 442000 sentences\n",
      "processed 442500 sentences\n",
      "processed 443000 sentences\n",
      "processed 443500 sentences\n",
      "processed 444000 sentences\n",
      "processed 444500 sentences\n",
      "processed 445000 sentences\n",
      "processed 445500 sentences\n",
      "processed 446000 sentences\n",
      "processed 446500 sentences\n",
      "processed 447000 sentences\n",
      "processed 447500 sentences\n",
      "processed 448000 sentences\n",
      "processed 448500 sentences\n",
      "processed 449000 sentences\n",
      "processed 449500 sentences\n",
      "processed 450000 sentences\n",
      "processed 450500 sentences\n",
      "processed 451000 sentences\n",
      "processed 451500 sentences\n",
      "processed 452000 sentences\n",
      "processed 452500 sentences\n",
      "processed 453000 sentences\n",
      "processed 453500 sentences\n",
      "processed 454000 sentences\n",
      "processed 454500 sentences\n",
      "processed 455000 sentences\n",
      "processed 455500 sentences\n",
      "processed 456000 sentences\n",
      "processed 456500 sentences\n",
      "processed 457000 sentences\n",
      "processed 457500 sentences\n",
      "processed 458000 sentences\n",
      "processed 458500 sentences\n",
      "processed 459000 sentences\n",
      "processed 459500 sentences\n",
      "processed 460000 sentences\n",
      "processed 460500 sentences\n",
      "processed 461000 sentences\n",
      "processed 461500 sentences\n",
      "processed 462000 sentences\n",
      "potential offset issue  multicardioner_test+bg_4085 Pirazinamide - Etambutolo Pirazinamide\n",
      "- Etambutolo\n",
      "processed 462500 sentences\n",
      "processed 463000 sentences\n",
      "processed 463500 sentences\n",
      "processed 464000 sentences\n",
      "processed 464500 sentences\n",
      "processed 465000 sentences\n",
      "processed 465500 sentences\n",
      "processed 466000 sentences\n",
      "processed 466500 sentences\n",
      "processed 467000 sentences\n",
      "processed 467500 sentences\n",
      "processed 468000 sentences\n",
      "processed 468500 sentences\n",
      "processed 469000 sentences\n",
      "potential offset issue  multicardioner_test+bg_4406 Cloxacillina - Cloxacillina\n",
      "-\n",
      "potential offset issue  multicardioner_test+bg_4406 Septrim forte - Rifampicina Septrim forte\n",
      "- Rifampicina\n",
      "processed 469500 sentences\n",
      "processed 470000 sentences\n",
      "processed 470500 sentences\n",
      "processed 471000 sentences\n",
      "processed 471500 sentences\n",
      "processed 472000 sentences\n",
      "processed 472500 sentences\n",
      "processed 473000 sentences\n",
      "processed 473500 sentences\n",
      "processed 474000 sentences\n",
      "processed 474500 sentences\n",
      "processed 475000 sentences\n",
      "processed 475500 sentences\n",
      "processed 476000 sentences\n",
      "processed 476500 sentences\n",
      "processed 477000 sentences\n",
      "processed 477500 sentences\n",
      "processed 478000 sentences\n",
      "processed 478500 sentences\n",
      "processed 479000 sentences\n",
      "processed 479500 sentences\n",
      "processed 480000 sentences\n",
      "processed 480500 sentences\n",
      "processed 481000 sentences\n",
      "processed 481500 sentences\n",
      "processed 482000 sentences\n",
      "processed 482500 sentences\n",
      "processed 483000 sentences\n",
      "processed 483500 sentences\n",
      "processed 484000 sentences\n",
      "processed 484500 sentences\n",
      "processed 485000 sentences\n",
      "processed 485500 sentences\n",
      "processed 486000 sentences\n",
      "potential offset issue  multicardioner_test+bg_5073 G - CSF G - CSF G-CSF\n",
      "G-CSF\n",
      "processed 486500 sentences\n",
      "processed 487000 sentences\n",
      "processed 487500 sentences\n",
      "processed 488000 sentences\n",
      "processed 488500 sentences\n",
      "processed 489000 sentences\n",
      "processed 489500 sentences\n",
      "processed 490000 sentences\n",
      "processed 490500 sentences\n",
      "processed 491000 sentences\n",
      "processed 491500 sentences\n",
      "processed 492000 sentences\n",
      "processed 492500 sentences\n",
      "processed 493000 sentences\n",
      "processed 493500 sentences\n",
      "processed 494000 sentences\n",
      "processed 494500 sentences\n",
      "processed 495000 sentences\n",
      "processed 495500 sentences\n",
      "processed 496000 sentences\n",
      "processed 496500 sentences\n",
      "processed 497000 sentences\n",
      "processed 497500 sentences\n",
      "processed 498000 sentences\n",
      "potential offset issue  multicardioner_test+bg_5571 Rifampicina S Isoniazide Rifampicina S\n",
      "Isoniazide\n",
      "potential offset issue  multicardioner_test+bg_5571 Etambutolo S Pirazinamide Etambutolo S\n",
      "Pirazinamide\n",
      "processed 498500 sentences\n",
      "processed 499000 sentences\n",
      "processed 499500 sentences\n",
      "processed 500000 sentences\n",
      "potential offset issue  multicardioner_test+bg_5655 IM Prednisone IM\n",
      "Prednisone\n",
      "processed 500500 sentences\n",
      "processed 501000 sentences\n",
      "processed 501500 sentences\n",
      "processed 502000 sentences\n",
      "processed 502500 sentences\n",
      "processed 503000 sentences\n",
      "processed 503500 sentences\n",
      "processed 504000 sentences\n",
      "processed 504500 sentences\n",
      "processed 505000 sentences\n",
      "processed 505500 sentences\n",
      "processed 506000 sentences\n",
      "processed 506500 sentences\n",
      "processed 507000 sentences\n",
      "processed 507500 sentences\n",
      "processed 508000 sentences\n",
      "processed 508500 sentences\n",
      "processed 509000 sentences\n",
      "processed 509500 sentences\n",
      "processed 510000 sentences\n",
      "processed 510500 sentences\n",
      "processed 511000 sentences\n",
      "processed 511500 sentences\n",
      "processed 512000 sentences\n",
      "processed 512500 sentences\n",
      "processed 513000 sentences\n",
      "processed 513500 sentences\n",
      "processed 514000 sentences\n",
      "processed 514500 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 515000 sentences\n",
      "processed 515500 sentences\n",
      "processed 516000 sentences\n",
      "processed 516500 sentences\n",
      "processed 517000 sentences\n",
      "processed 517500 sentences\n",
      "processed 518000 sentences\n",
      "processed 518500 sentences\n",
      "processed 519000 sentences\n",
      "processed 519500 sentences\n",
      "processed 520000 sentences\n",
      "processed 520500 sentences\n",
      "processed 521000 sentences\n",
      "potential offset issue  multicardioner_test+bg_655 Ampicillina / sulbactam Claritromicina Ampicillina / sulbactam\n",
      "Claritromicina\n",
      "processed 521500 sentences\n",
      "processed 522000 sentences\n",
      "processed 522500 sentences\n",
      "processed 523000 sentences\n",
      "processed 523500 sentences\n",
      "processed 524000 sentences\n",
      "processed 524500 sentences\n",
      "potential offset issue  multicardioner_test+bg_6691 13 - Fluconazolo 13\n",
      "- Fluconazolo\n",
      "potential offset issue  multicardioner_test+bg_6691 Fluconazolo a - Fluconazolo a\n",
      "-\n",
      "processed 525000 sentences\n",
      "processed 525500 sentences\n",
      "processed 526000 sentences\n",
      "processed 526500 sentences\n",
      "processed 527000 sentences\n",
      "processed 527500 sentences\n",
      "processed 528000 sentences\n",
      "processed 528500 sentences\n",
      "processed 529000 sentences\n",
      "processed 529500 sentences\n",
      "processed 530000 sentences\n",
      "processed 530500 sentences\n",
      "processed 531000 sentences\n",
      "processed 531500 sentences\n",
      "processed 532000 sentences\n",
      "processed 532500 sentences\n",
      "processed 533000 sentences\n",
      "processed 533500 sentences\n",
      "potential offset issue  multicardioner_test+bg_7055 vitamina B - 12 B - 12 vitamina B-12\n",
      "B-12\n",
      "processed 534000 sentences\n",
      "processed 534500 sentences\n",
      "processed 535000 sentences\n",
      "processed 535500 sentences\n",
      "processed 536000 sentences\n",
      "processed 536500 sentences\n",
      "processed 537000 sentences\n",
      "processed 537500 sentences\n",
      "processed 538000 sentences\n",
      "processed 538500 sentences\n",
      "processed 539000 sentences\n",
      "processed 539500 sentences\n",
      "processed 540000 sentences\n",
      "processed 540500 sentences\n",
      "processed 541000 sentences\n",
      "processed 541500 sentences\n",
      "processed 542000 sentences\n",
      "processed 542500 sentences\n",
      "processed 543000 sentences\n",
      "processed 543500 sentences\n",
      "processed 544000 sentences\n",
      "processed 544500 sentences\n",
      "processed 545000 sentences\n",
      "processed 545500 sentences\n",
      "processed 546000 sentences\n",
      "processed 546500 sentences\n",
      "processed 547000 sentences\n",
      "potential offset issue  multicardioner_test+bg_7554 Bevacizumab A Bevacizumab\n",
      "A\n",
      "processed 547500 sentences\n",
      "processed 548000 sentences\n",
      "processed 548500 sentences\n",
      "processed 549000 sentences\n",
      "processed 549500 sentences\n",
      "processed 550000 sentences\n",
      "processed 550500 sentences\n",
      "processed 551000 sentences\n",
      "processed 551500 sentences\n",
      "processed 552000 sentences\n",
      "processed 552500 sentences\n",
      "processed 553000 sentences\n",
      "processed 553500 sentences\n",
      "processed 554000 sentences\n",
      "processed 554500 sentences\n",
      "processed 555000 sentences\n",
      "potential offset issue  multicardioner_test+bg_7775 losartan losartan losartan\n",
      "losartan\n",
      "potential offset issue  multicardioner_test+bg_7775 dalteparina warfarin dalteparina\n",
      "warfarin\n",
      "processed 555500 sentences\n",
      "processed 556000 sentences\n",
      "processed 556500 sentences\n",
      "processed 557000 sentences\n",
      "processed 557500 sentences\n",
      "processed 558000 sentences\n",
      "processed 558500 sentences\n",
      "processed 559000 sentences\n",
      "processed 559500 sentences\n",
      "processed 560000 sentences\n",
      "processed 560500 sentences\n",
      "processed 561000 sentences\n",
      "processed 561500 sentences\n",
      "processed 562000 sentences\n",
      "processed 562500 sentences\n",
      "processed 563000 sentences\n",
      "processed 563500 sentences\n"
     ]
    }
   ],
   "source": [
    "original_mentions_list = []\n",
    "for index, row in df_dev_sentences[df_dev_sentences['lang']==lang].iterrows():\n",
    "    text = row['text'].rstrip()\n",
    "    tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "    #doc = nlp(tokenized_sentence)\n",
    "    #phrase_mentions = get_phrase_mentions(doc)\n",
    "    filtered_mentions = get_mentions(tokenized_sentence)\n",
    "    \n",
    "    #filtered_mentions = get_filtered_mentions(doc, phrase_mentions, mentions)\n",
    "    \n",
    "    original_mentions = get_original_mention_offset(filtered_mentions, tokenized_sentence, original_token_offsets, text, row['filename'], row['batch_start'])\n",
    "    original_mentions_list.extend(original_mentions)\n",
    "    \n",
    "    if (index+1) % 500 == 0:\n",
    "        print(f'processed {index+1} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>start_span</th>\n",
       "      <th>end_span</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>812</td>\n",
       "      <td>820</td>\n",
       "      <td>ossigeno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>821</td>\n",
       "      <td>832</td>\n",
       "      <td>umidificato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>1250</td>\n",
       "      <td>1258</td>\n",
       "      <td>fentanil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>1818</td>\n",
       "      <td>1827</td>\n",
       "      <td>midazolam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>1837</td>\n",
       "      <td>1844</td>\n",
       "      <td>morfina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  start_span  end_span         text\n",
       "0  multicardioner_test+bg_100         812       820     ossigeno\n",
       "1  multicardioner_test+bg_100         821       832  umidificato\n",
       "2  multicardioner_test+bg_100        1250      1258     fentanil\n",
       "3  multicardioner_test+bg_100        1818      1827    midazolam\n",
       "4  multicardioner_test+bg_100        1837      1844      morfina"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mentions = pd.DataFrame.from_records(original_mentions_list)\n",
    "df_mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions['label'] = target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions[['filename', 'label', 'start_span', 'end_span', 'text']].to_csv(result_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/ and w/o filtering\n",
    "# es - |0.8636|0.9028|0.8827 || |0.8479|0.9104|0.878\n",
    "# en - |0.8507|0.8924|0.8711 || |0.832|0.898|0.8638\n",
    "# it - |0.8606|0.8789|0.8697 || |0.8414|0.8925|0.8662"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
