{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RXA3_zObUkl",
    "outputId": "41533b0a-3871-4ceb-ee93-4f4e9fe6350c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "tqdm.pandas()\n",
    "metric = evaluate.load('seqeval')\n",
    "map = {0: \"O\", 1: \"B-FARMACO\", 2: \"I-FARMACO\"}\n",
    "reverse_map = {'O':0, 'B-FARMACO': 1, 'I-FARMACO': 2}\n",
    "tokenizer_kwargs = {'padding': True, 'truncation':True, 'max_length':512}\n",
    "\n",
    "task = \"ner\"\n",
    "#base_model = 'aaaksenova/xlmr_medical'\n",
    "#output_path = 'output/models'\n",
    "#model_checkpoint = f\"aaaksenova/xlmr_drug_classifier\"\n",
    "num_labels = 3\n",
    "target_label = 'FARMACO'\n",
    "\n",
    "#aaaksenova/xlmr_medical - for just xlmr\n",
    "#aaaksenova/xlmr_drug_classifier + aaaksenova/xlmr_medical for filtering + xlmr\n",
    "#Spanish: aaaksenova/SpanishRoberta_multicardioner , \n",
    "#English: aaaksenova/BioLinkBert_multicardioner , \n",
    "#Italian: aaaksenova/SpanishRoberta_it_medprocner\n",
    "MODEL_CLASS = \"aaaksenova/xlmr_drug_classifier\"\n",
    "MODEL_NER = \"aaaksenova/xlmr_medical\"\n",
    "use_filtering = True\n",
    "test_name = 'test' #dev\n",
    "lang = 'en'\n",
    "\n",
    "model_name = MODEL_NER[MODEL_NER.index('/')+1:]\n",
    "sentences_file = f'data/{test_name}_sentences.tsv'\n",
    "result_file = f'output/multicardioner_track2_{test_name}_{lang}_predictions_{model_name}_{use_filtering}.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output/multicardioner_track2_test_en_predictions_xlmr_medical_True.tsv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>batch_number</th>\n",
       "      <th>batch_start</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Setting: primary care (PC).\\n</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>Reason for consultation: 26-year-old woman who...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>She had been seen at the PC emergency centre f...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>4</td>\n",
       "      <td>197</td>\n",
       "      <td>She explained abdominal pain of 2 weeks' evolu...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>5</td>\n",
       "      <td>333</td>\n",
       "      <td>Clinical history: personal history of no inter...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   filename  batch_number  batch_start  \\\n",
       "0  multicardioner_test+bg_1             1            0   \n",
       "1  multicardioner_test+bg_1             2           28   \n",
       "2  multicardioner_test+bg_1             3          132   \n",
       "3  multicardioner_test+bg_1             4          197   \n",
       "4  multicardioner_test+bg_1             5          333   \n",
       "\n",
       "                                                text lang  \n",
       "0                      Setting: primary care (PC).\\n   en  \n",
       "1  Reason for consultation: 26-year-old woman who...   en  \n",
       "2  She had been seen at the PC emergency centre f...   en  \n",
       "3  She explained abdominal pain of 2 weeks' evolu...   en  \n",
       "4  Clinical history: personal history of no inter...   en  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_sentences = pd.read_csv(sentences_file, sep='\\t', keep_default_na=False)\n",
    "df_dev_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKENIZATION_REGEX = re.compile(r\"([0-9\\w]+|[^0-9\\w])\", re.UNICODE) # ’ - es, ' - en, ' - it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    original_token_offsets = []\n",
    "\n",
    "    offset = 0\n",
    "    new_offset = 0\n",
    "    nonspace_token_seen = False\n",
    "\n",
    "    tokens = [t for t in TOKENIZATION_REGEX.split(text) if t]\n",
    "    for t in tokens:\n",
    "        if not t.isspace():\n",
    "            original_token_offsets.append([offset, offset + len(t), t, new_offset, new_offset + len(t)])\n",
    "            nonspace_token_seen = True\n",
    "            new_offset += len(t) + 1\n",
    "        offset += len(t)\n",
    "        \n",
    "\n",
    "    tokenized_sentence = ' '.join([l[2] for l in original_token_offsets])\n",
    "\n",
    "    # store original token offsets\n",
    "    # pass the tokenized string for prediction\n",
    "    return tokenized_sentence, original_token_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylvia/.local/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task=\"text-classification\", model=MODEL_CLASS, device='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NER)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NER).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 512\n",
    "ner_pipe = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", stride=0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'FARMACO',\n",
       "  'score': 0.9957895,\n",
       "  'word': 'Cefazolin2g',\n",
       "  'start': 0,\n",
       "  'end': 11},\n",
       " {'entity_group': 'FARMACO',\n",
       "  'score': 0.9987437,\n",
       "  'word': 'gentamicin',\n",
       "  'start': 23,\n",
       "  'end': 33},\n",
       " {'entity_group': 'FARMACO',\n",
       "  'score': 0.99871254,\n",
       "  'word': 'rifampicin',\n",
       "  'start': 49,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe('Cefazolin2g c/8 hs iv; gentamicin 3mg/kg/day iv; rifampicin 600 mg c/12 hsvo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RjIURpjUvxLW"
   },
   "outputs": [],
   "source": [
    "# group annotations around a clinical procedure mention, based on the annotation label\n",
    "def group_annotations_strict(annotations):\n",
    "    groups = []\n",
    "    i = 0\n",
    "    while i < len(annotations):\n",
    "        if annotations[i]['entity_group'] == 'LABEL_0':\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        group = [] # for the strict strategy, a group is a B (or many Bs), followed by 1 or more Is\n",
    "        if annotations[i]['entity_group'] == 'LABEL_1':\n",
    "            group.append(annotations[i])\n",
    "            i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_1'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_2'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            groups.append(group)\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SsntbSAryqAn"
   },
   "outputs": [],
   "source": [
    "# merge grouped annotations to form a complete entity mention\n",
    "def merge_annotation_group_entries(annotation_group, sentence):\n",
    "    start = annotation_group[0]['start']\n",
    "    end = annotation_group[len(annotation_group) - 1]['end']\n",
    "    text = sentence[start:end]\n",
    "    return {'start': start, 'end': end, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8f574bI6qEwE"
   },
   "outputs": [],
   "source": [
    "def get_mentions(sentence):\n",
    "    if use_filtering:\n",
    "        out = classifier(sentence, **tokenizer_kwargs)\n",
    "    #print(out[0]['label'] == 'ent')\n",
    "    if not use_filtering or out[0]['label'] == \"ent\":\n",
    "        ner_result = ner_pipe(sentence)\n",
    "        return [{'start': mention['start'], 'end': mention['end'], 'text': mention['word']} for mention in ner_result]\n",
    "        #annotation_groups = group_annotations_strict(ner_result)        \n",
    "        #return [merge_annotation_group_entries(group, sentence) for group in annotation_groups]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_mention_offset(mentions, sentence, original_token_offsets, original_sentence, filename, batch_start):\n",
    "    original_mention_offsets = []\n",
    "    current_offset_idx = 0 \n",
    "    for mention in mentions:\n",
    "        start = mention['start']\n",
    "        end = mention['end']\n",
    "        \n",
    "        original_start = -1\n",
    "        original_end = -1\n",
    "        while current_offset_idx < len(original_token_offsets):\n",
    "            token = original_token_offsets[current_offset_idx]\n",
    "            \n",
    "            if token[3] <= start:\n",
    "                original_start = token[0]\n",
    "            \n",
    "            if token[4] >= end:\n",
    "                original_end = token[1]\n",
    "                break\n",
    "            \n",
    "            current_offset_idx += 1\n",
    "\n",
    "        sentence_no_spaces = sentence[start:end].replace(' ', '')\n",
    "        original_sentence_no_spaces = original_sentence[original_start:original_end].replace(' ', '')\n",
    "        # check whether the detected span is contained in the original\n",
    "        if sentence_no_spaces != original_sentence_no_spaces and not(sentence_no_spaces in original_sentence_no_spaces):\n",
    "            print('potential offset issue ', filename, sentence[start:end], original_sentence[original_start:original_end])\n",
    "        if original_start == -1 or original_end == -1:\n",
    "            print('mention not found ', filename, mention)\n",
    "            \n",
    "        original_mention_offsets.append({\n",
    "            'filename': filename, \n",
    "            'start_span':original_start+batch_start, \n",
    "            'end_span':original_end+batch_start, \n",
    "            'text': original_sentence[original_start:original_end]\n",
    "        })\n",
    "    \n",
    "    return original_mention_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Treatment was started with piperacillin tazobactam 4.5g iv every 8h (after taking blood cultures) for 4 days.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Treatment was started with piperacillin tazobactam 4 . 5g iv every 8h ( after taking blood cultures ) for 4 days .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'FARMACO',\n",
       "  'score': 0.9985007,\n",
       "  'word': 'piperacillin tazobactam',\n",
       "  'start': 27,\n",
       "  'end': 50}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb43SpEHQsI9",
    "outputId": "40405391-c271-4057-ba9f-457c8d2f8528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 27, 'end': 50, 'text': 'piperacillin tazobactam'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = get_mentions(tokenized_sentence)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'filename',\n",
       "  'start_span': 37,\n",
       "  'end_span': 60,\n",
       "  'text': 'piperacillin tazobactam'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_original_mention_offset(mentions, tokenized_sentence, original_token_offsets, text, 'filename', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_spans(doc, mentions):\n",
    "    spans = []\n",
    "    current_offset_idx = 0 \n",
    "    for mention in mentions:\n",
    "        start = mention['start']\n",
    "        end = mention['end']\n",
    "        \n",
    "        span_start = -1\n",
    "        span_end = -1\n",
    "        while current_offset_idx < len(doc):\n",
    "            token = doc[current_offset_idx]\n",
    "            \n",
    "            if token.idx <= start:\n",
    "                span_start = token.idx\n",
    "            \n",
    "            if token.idx + len(token) >= end:\n",
    "                span_end = token.idx + len(token)\n",
    "                break\n",
    "            \n",
    "            current_offset_idx += 1\n",
    "        \n",
    "        spans.append(doc.char_span(span_start, span_end))\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "def get_filtered_mentions(doc, phrase_mentions, mentions):\n",
    "    span_mentions = get_mention_spans(doc, mentions)\n",
    "    #phrase_mentions.extend(span_mentions)\n",
    "    filtered_matches = filter_spans(span_mentions)\n",
    "    return [{'start': doc[match.start].idx,'end': doc[match.end-1].idx + len(doc[match.end-1]), 'text':doc[match.start:match.end]} for match in filtered_matches if len(match) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylvia/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 500 sentences\n",
      "processed 1000 sentences\n",
      "processed 1500 sentences\n",
      "processed 2000 sentences\n",
      "processed 2500 sentences\n",
      "processed 3000 sentences\n",
      "processed 3500 sentences\n",
      "processed 4000 sentences\n",
      "processed 4500 sentences\n",
      "processed 5000 sentences\n",
      "potential offset issue  multicardioner_test+bg_1200 SF 0 . 9 % Carbamazepine SF 0.9%\n",
      "Carbamazepine\n",
      "processed 5500 sentences\n",
      "processed 6000 sentences\n",
      "processed 6500 sentences\n",
      "processed 7000 sentences\n",
      "processed 7500 sentences\n",
      "processed 8000 sentences\n",
      "processed 8500 sentences\n",
      "processed 9000 sentences\n",
      "processed 9500 sentences\n",
      "processed 10000 sentences\n",
      "processed 10500 sentences\n",
      "processed 11000 sentences\n",
      "processed 11500 sentences\n",
      "processed 12000 sentences\n",
      "processed 12500 sentences\n",
      "processed 13000 sentences\n",
      "processed 13500 sentences\n",
      "processed 14000 sentences\n",
      "processed 14500 sentences\n",
      "processed 15000 sentences\n",
      "processed 15500 sentences\n",
      "processed 16000 sentences\n",
      "processed 16500 sentences\n",
      "processed 17000 sentences\n",
      "processed 17500 sentences\n",
      "processed 18000 sentences\n",
      "processed 18500 sentences\n",
      "processed 19000 sentences\n",
      "processed 19500 sentences\n",
      "processed 20000 sentences\n",
      "processed 20500 sentences\n",
      "processed 21000 sentences\n",
      "processed 21500 sentences\n",
      "processed 22000 sentences\n",
      "processed 22500 sentences\n",
      "processed 23000 sentences\n",
      "processed 23500 sentences\n",
      "processed 24000 sentences\n",
      "processed 24500 sentences\n",
      "processed 25000 sentences\n",
      "processed 25500 sentences\n",
      "processed 26000 sentences\n",
      "processed 26500 sentences\n",
      "processed 27000 sentences\n",
      "processed 27500 sentences\n",
      "processed 28000 sentences\n",
      "processed 28500 sentences\n",
      "processed 29000 sentences\n",
      "processed 29500 sentences\n",
      "processed 30000 sentences\n",
      "processed 30500 sentences\n",
      "processed 31000 sentences\n",
      "processed 31500 sentences\n",
      "processed 32000 sentences\n",
      "processed 32500 sentences\n",
      "processed 33000 sentences\n",
      "processed 33500 sentences\n",
      "processed 34000 sentences\n",
      "processed 34500 sentences\n",
      "processed 35000 sentences\n",
      "processed 35500 sentences\n",
      "processed 36000 sentences\n",
      "processed 36500 sentences\n",
      "processed 37000 sentences\n",
      "processed 37500 sentences\n",
      "processed 38000 sentences\n",
      "processed 38500 sentences\n",
      "processed 39000 sentences\n",
      "processed 39500 sentences\n",
      "processed 40000 sentences\n",
      "processed 40500 sentences\n",
      "processed 41000 sentences\n",
      "processed 41500 sentences\n",
      "processed 42000 sentences\n",
      "processed 42500 sentences\n",
      "processed 43000 sentences\n",
      "processed 43500 sentences\n",
      "processed 44000 sentences\n",
      "processed 44500 sentences\n",
      "processed 45000 sentences\n",
      "processed 45500 sentences\n",
      "processed 46000 sentences\n",
      "processed 46500 sentences\n",
      "processed 47000 sentences\n",
      "processed 47500 sentences\n",
      "processed 48000 sentences\n",
      "processed 48500 sentences\n",
      "processed 49000 sentences\n",
      "processed 49500 sentences\n",
      "processed 50000 sentences\n",
      "processed 50500 sentences\n",
      "potential offset issue  multicardioner_test+bg_2906 2 - Isentress 2\n",
      "- Isentress\n",
      "potential offset issue  multicardioner_test+bg_2906 1 - Hydroxyl 1\n",
      "- Hydroxyl\n",
      "processed 51000 sentences\n",
      "processed 51500 sentences\n",
      "processed 52000 sentences\n",
      "processed 52500 sentences\n",
      "processed 53000 sentences\n",
      "processed 53500 sentences\n",
      "processed 54000 sentences\n",
      "processed 54500 sentences\n",
      "processed 55000 sentences\n",
      "processed 55500 sentences\n",
      "processed 56000 sentences\n",
      "processed 56500 sentences\n",
      "processed 57000 sentences\n",
      "processed 57500 sentences\n",
      "processed 58000 sentences\n",
      "processed 58500 sentences\n",
      "processed 59000 sentences\n",
      "processed 59500 sentences\n",
      "processed 60000 sentences\n",
      "processed 60500 sentences\n",
      "processed 61000 sentences\n",
      "processed 61500 sentences\n",
      "processed 62000 sentences\n",
      "processed 62500 sentences\n",
      "processed 63000 sentences\n",
      "processed 63500 sentences\n",
      "processed 64000 sentences\n",
      "processed 64500 sentences\n",
      "processed 65000 sentences\n",
      "processed 65500 sentences\n",
      "processed 66000 sentences\n",
      "processed 66500 sentences\n",
      "processed 67000 sentences\n",
      "processed 67500 sentences\n",
      "processed 68000 sentences\n",
      "processed 68500 sentences\n",
      "processed 69000 sentences\n",
      "processed 69500 sentences\n",
      "processed 70000 sentences\n",
      "processed 70500 sentences\n",
      "processed 71000 sentences\n",
      "processed 71500 sentences\n",
      "processed 72000 sentences\n",
      "processed 72500 sentences\n",
      "processed 73000 sentences\n",
      "processed 73500 sentences\n",
      "processed 74000 sentences\n",
      "processed 74500 sentences\n",
      "processed 75000 sentences\n",
      "processed 75500 sentences\n",
      "processed 76000 sentences\n",
      "processed 76500 sentences\n",
      "processed 77000 sentences\n",
      "processed 77500 sentences\n",
      "processed 78000 sentences\n",
      "processed 78500 sentences\n",
      "processed 79000 sentences\n",
      "processed 79500 sentences\n",
      "processed 80000 sentences\n",
      "processed 80500 sentences\n",
      "processed 81000 sentences\n",
      "potential offset issue  multicardioner_test+bg_4085 Pyridoxine - Rifampicin - Pyrazinamide Pyridoxine - Rifampicin\n",
      "- Pyrazinamide\n",
      "processed 81500 sentences\n",
      "processed 82000 sentences\n",
      "processed 82500 sentences\n",
      "processed 83000 sentences\n",
      "processed 83500 sentences\n",
      "processed 84000 sentences\n",
      "processed 84500 sentences\n",
      "processed 85000 sentences\n",
      "processed 85500 sentences\n",
      "processed 86000 sentences\n",
      "processed 86500 sentences\n",
      "processed 87000 sentences\n",
      "processed 87500 sentences\n",
      "processed 88000 sentences\n",
      "processed 88500 sentences\n",
      "processed 89000 sentences\n",
      "processed 89500 sentences\n",
      "processed 90000 sentences\n",
      "processed 90500 sentences\n",
      "processed 91000 sentences\n",
      "processed 91500 sentences\n",
      "processed 92000 sentences\n",
      "processed 92500 sentences\n",
      "processed 93000 sentences\n",
      "processed 93500 sentences\n",
      "processed 94000 sentences\n",
      "processed 94500 sentences\n",
      "processed 95000 sentences\n",
      "processed 95500 sentences\n",
      "processed 96000 sentences\n",
      "processed 96500 sentences\n",
      "processed 97000 sentences\n",
      "processed 97500 sentences\n",
      "processed 98000 sentences\n",
      "processed 98500 sentences\n",
      "processed 99000 sentences\n",
      "processed 99500 sentences\n",
      "processed 100000 sentences\n",
      "processed 100500 sentences\n",
      "processed 101000 sentences\n",
      "processed 101500 sentences\n",
      "processed 102000 sentences\n",
      "processed 102500 sentences\n",
      "processed 103000 sentences\n",
      "processed 103500 sentences\n",
      "processed 104000 sentences\n",
      "processed 104500 sentences\n",
      "processed 105000 sentences\n",
      "processed 105500 sentences\n",
      "processed 106000 sentences\n",
      "processed 106500 sentences\n",
      "processed 107000 sentences\n",
      "processed 107500 sentences\n",
      "processed 108000 sentences\n",
      "processed 108500 sentences\n",
      "processed 109000 sentences\n",
      "processed 109500 sentences\n",
      "processed 110000 sentences\n",
      "processed 110500 sentences\n",
      "processed 111000 sentences\n",
      "processed 111500 sentences\n",
      "processed 112000 sentences\n",
      "processed 112500 sentences\n",
      "processed 113000 sentences\n",
      "processed 113500 sentences\n",
      "processed 114000 sentences\n",
      "processed 114500 sentences\n",
      "processed 115000 sentences\n",
      "processed 115500 sentences\n",
      "processed 116000 sentences\n",
      "processed 116500 sentences\n",
      "processed 117000 sentences\n",
      "processed 117500 sentences\n",
      "processed 118000 sentences\n",
      "processed 118500 sentences\n",
      "processed 119000 sentences\n",
      "processed 119500 sentences\n",
      "potential offset issue  multicardioner_test+bg_5571 Streptomycin S Rifampicin S Isoniazid S Ethambutol S Pyrazinamide Streptomycin S\n",
      "Rifampicin S\n",
      "Isoniazid S\n",
      "Ethambutol S\n",
      "Pyrazinamide\n",
      "processed 120000 sentences\n",
      "processed 120500 sentences\n",
      "processed 121000 sentences\n",
      "processed 121500 sentences\n",
      "processed 122000 sentences\n",
      "processed 122500 sentences\n",
      "processed 123000 sentences\n",
      "processed 123500 sentences\n",
      "processed 124000 sentences\n",
      "processed 124500 sentences\n",
      "processed 125000 sentences\n",
      "processed 125500 sentences\n",
      "processed 126000 sentences\n",
      "processed 126500 sentences\n",
      "processed 127000 sentences\n",
      "processed 127500 sentences\n",
      "processed 128000 sentences\n",
      "processed 128500 sentences\n",
      "processed 129000 sentences\n",
      "processed 129500 sentences\n",
      "processed 130000 sentences\n",
      "processed 130500 sentences\n",
      "processed 131000 sentences\n",
      "processed 131500 sentences\n",
      "processed 132000 sentences\n",
      "processed 132500 sentences\n",
      "processed 133000 sentences\n",
      "processed 133500 sentences\n",
      "processed 134000 sentences\n",
      "processed 134500 sentences\n",
      "processed 135000 sentences\n",
      "processed 135500 sentences\n",
      "processed 136000 sentences\n",
      "processed 136500 sentences\n",
      "processed 137000 sentences\n",
      "processed 137500 sentences\n",
      "processed 138000 sentences\n",
      "processed 138500 sentences\n",
      "processed 139000 sentences\n",
      "processed 139500 sentences\n",
      "processed 140000 sentences\n",
      "processed 140500 sentences\n",
      "processed 141000 sentences\n",
      "processed 141500 sentences\n",
      "processed 142000 sentences\n",
      "processed 142500 sentences\n",
      "processed 143000 sentences\n",
      "processed 143500 sentences\n",
      "processed 144000 sentences\n",
      "processed 144500 sentences\n",
      "processed 145000 sentences\n",
      "processed 145500 sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 146000 sentences\n",
      "processed 146500 sentences\n",
      "processed 147000 sentences\n",
      "processed 147500 sentences\n",
      "processed 148000 sentences\n",
      "potential offset issue  multicardioner_test+bg_6691 13 - Fluconazole 13\n",
      "- Fluconazole\n",
      "processed 148500 sentences\n",
      "processed 149000 sentences\n",
      "processed 149500 sentences\n",
      "processed 150000 sentences\n",
      "processed 150500 sentences\n",
      "processed 151000 sentences\n",
      "processed 151500 sentences\n",
      "processed 152000 sentences\n",
      "processed 152500 sentences\n",
      "processed 153000 sentences\n",
      "processed 153500 sentences\n",
      "processed 154000 sentences\n",
      "processed 154500 sentences\n",
      "processed 155000 sentences\n",
      "processed 155500 sentences\n",
      "processed 156000 sentences\n",
      "processed 156500 sentences\n",
      "processed 157000 sentences\n",
      "processed 157500 sentences\n",
      "processed 158000 sentences\n",
      "processed 158500 sentences\n",
      "processed 159000 sentences\n",
      "processed 159500 sentences\n",
      "processed 160000 sentences\n",
      "processed 160500 sentences\n",
      "processed 161000 sentences\n",
      "processed 161500 sentences\n",
      "processed 162000 sentences\n",
      "processed 162500 sentences\n",
      "processed 163000 sentences\n",
      "processed 163500 sentences\n",
      "processed 164000 sentences\n",
      "processed 164500 sentences\n",
      "processed 165000 sentences\n",
      "processed 165500 sentences\n",
      "processed 166000 sentences\n",
      "processed 166500 sentences\n",
      "processed 167000 sentences\n",
      "processed 167500 sentences\n",
      "processed 168000 sentences\n",
      "processed 168500 sentences\n",
      "processed 169000 sentences\n",
      "processed 169500 sentences\n",
      "processed 170000 sentences\n",
      "processed 170500 sentences\n",
      "processed 171000 sentences\n",
      "processed 171500 sentences\n",
      "processed 172000 sentences\n",
      "processed 172500 sentences\n",
      "processed 173000 sentences\n",
      "processed 173500 sentences\n",
      "processed 174000 sentences\n",
      "processed 174500 sentences\n",
      "processed 175000 sentences\n",
      "processed 175500 sentences\n",
      "processed 176000 sentences\n",
      "processed 176500 sentences\n",
      "processed 177000 sentences\n",
      "processed 177500 sentences\n",
      "processed 178000 sentences\n",
      "processed 178500 sentences\n",
      "processed 179000 sentences\n",
      "processed 179500 sentences\n",
      "processed 180000 sentences\n",
      "processed 180500 sentences\n",
      "processed 181000 sentences\n",
      "processed 181500 sentences\n",
      "processed 182000 sentences\n",
      "potential offset issue  multicardioner_test+bg_7851 phosphodiesterase − 3 inhibitor phosphodiesterase − 3 inhibitor\n",
      "processed 182500 sentences\n",
      "processed 183000 sentences\n",
      "processed 183500 sentences\n",
      "processed 184000 sentences\n",
      "processed 184500 sentences\n",
      "processed 185000 sentences\n",
      "processed 185500 sentences\n",
      "processed 186000 sentences\n",
      "processed 186500 sentences\n",
      "processed 187000 sentences\n",
      "processed 187500 sentences\n",
      "processed 188000 sentences\n",
      "processed 188500 sentences\n"
     ]
    }
   ],
   "source": [
    "original_mentions_list = []\n",
    "\n",
    "for index, row in df_dev_sentences[df_dev_sentences['lang']==lang].iterrows():\n",
    "    text = row['text'].rstrip()\n",
    "    tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "    #doc = nlp(tokenized_sentence)\n",
    "    #phrase_mentions = get_phrase_mentions(doc)\n",
    "    \n",
    "    filtered_mentions = get_mentions(tokenized_sentence)\n",
    "    \n",
    "    #filtered_mentions = get_filtered_mentions(doc, phrase_mentions, mentions)\n",
    "    \n",
    "    original_mentions = get_original_mention_offset(filtered_mentions, tokenized_sentence, original_token_offsets, text, row['filename'], row['batch_start'])\n",
    "    original_mentions_list.extend(original_mentions)\n",
    "    \n",
    "    if (index+1) % 500 == 0:\n",
    "        print(f'processed {index+1} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>start_span</th>\n",
       "      <th>end_span</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>663</td>\n",
       "      <td>673</td>\n",
       "      <td>Humidified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>674</td>\n",
       "      <td>680</td>\n",
       "      <td>oxygen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>992</td>\n",
       "      <td>1000</td>\n",
       "      <td>fentanyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>1503</td>\n",
       "      <td>1512</td>\n",
       "      <td>midazolam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multicardioner_test+bg_100</td>\n",
       "      <td>1522</td>\n",
       "      <td>1530</td>\n",
       "      <td>morphine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  start_span  end_span        text\n",
       "0  multicardioner_test+bg_100         663       673  Humidified\n",
       "1  multicardioner_test+bg_100         674       680      oxygen\n",
       "2  multicardioner_test+bg_100         992      1000    fentanyl\n",
       "3  multicardioner_test+bg_100        1503      1512   midazolam\n",
       "4  multicardioner_test+bg_100        1522      1530    morphine"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mentions = pd.DataFrame.from_records(original_mentions_list)\n",
    "df_mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions['label'] = target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions[['filename', 'label', 'start_span', 'end_span', 'text']].to_csv(result_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w/ and w/o filtering\n",
    "# es - |0.8636|0.9028|0.8827 || |0.8479|0.9104|0.878\n",
    "# en - |0.8507|0.8924|0.8711 || |0.832|0.898|0.8638\n",
    "# it - |0.8606|0.8789|0.8697 || |0.8414|0.8925|0.8662"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
