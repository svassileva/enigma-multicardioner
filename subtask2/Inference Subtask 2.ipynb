{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TRv18xWQbOhr"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "task = \"ner\"\n",
    "base_model = 'aaaksenova/xlmr_medical'\n",
    "output_path = 'output/models'\n",
    "model_checkpoint = f\"aaaksenova/xlmr_drug_classifier\"\n",
    "num_labels = 3\n",
    "target_label = 'FARMACO'\n",
    "\n",
    "sentences_file = 'data/dev_sentences.tsv'\n",
    "lang = 'it'\n",
    "result_file = f'output/multicardioner_track2_cardioccc_dev_{lang}_predictions.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>batch_number</th>\n",
       "      <th>batch_start</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Anamnesis\\n</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Male, 79 years old.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>Self-assisted.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>From Salto.\\n</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "      <td>Pathological history: -Chronic arterial hypert...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  batch_number  batch_start  \\\n",
       "0  casos_clinicos_cardiologia10             1            0   \n",
       "1  casos_clinicos_cardiologia10             2           10   \n",
       "2  casos_clinicos_cardiologia10             3           30   \n",
       "3  casos_clinicos_cardiologia10             4           45   \n",
       "4  casos_clinicos_cardiologia10             5           57   \n",
       "\n",
       "                                                text lang  \n",
       "0                                        Anamnesis\\n   en  \n",
       "1                                Male, 79 years old.   en  \n",
       "2                                     Self-assisted.   en  \n",
       "3                                      From Salto.\\n   en  \n",
       "4  Pathological history: -Chronic arterial hypert...   en  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_sentences = pd.read_csv(sentences_file, sep='\\t')\n",
    "df_dev_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKENIZATION_REGEX = re.compile(r\"([0-9\\w]+|[^0-9\\w])\", re.UNICODE) # â€™ - es, ' - en, ' - it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    original_token_offsets = []\n",
    "\n",
    "    offset = 0\n",
    "    new_offset = 0\n",
    "    nonspace_token_seen = False\n",
    "\n",
    "    tokens = [t for t in TOKENIZATION_REGEX.split(text) if t]\n",
    "    for t in tokens:\n",
    "        if not t.isspace():\n",
    "            original_token_offsets.append([offset, offset + len(t), t, new_offset, new_offset + len(t)])\n",
    "            nonspace_token_seen = True\n",
    "            new_offset += len(t) + 1\n",
    "        offset += len(t)\n",
    "        \n",
    "\n",
    "    tokenized_sentence = ' '.join([l[2] for l in original_token_offsets])\n",
    "\n",
    "    # store original token offsets\n",
    "    # pass the tokenized string for prediction\n",
    "    return tokenized_sentence, original_token_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RXA3_zObUkl",
    "outputId": "41533b0a-3871-4ceb-ee93-4f4e9fe6350c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "tqdm.pandas()\n",
    "metric = evaluate.load('seqeval')\n",
    "map = {0: \"O\", 1: \"B-FARMACO\", 2: \"I-FARMACO\"}\n",
    "reverse_map = {'O':0, 'B-FARMACO': 1, 'I-FARMACO': 2}\n",
    "MODEL_CLASS = \"aaaksenova/xlmr_drug_classifier\"\n",
    "MODEL_NER = \"aaaksenova/xlmr_medical\"\n",
    "tokenizer_kwargs = {'padding': True, 'truncation':True, 'max_length':512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylvia/.local/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(task=\"text-classification\", model=MODEL_CLASS, device='cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NER)\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NER).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipe = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", stride=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'FARMACO',\n",
       "  'score': 0.99786454,\n",
       "  'word': 'Cefazolin',\n",
       "  'start': 0,\n",
       "  'end': 9},\n",
       " {'entity_group': 'FARMACO',\n",
       "  'score': 0.9980075,\n",
       "  'word': 'gentamicin',\n",
       "  'start': 23,\n",
       "  'end': 33},\n",
       " {'entity_group': 'FARMACO',\n",
       "  'score': 0.99813527,\n",
       "  'word': 'rifampicin',\n",
       "  'start': 49,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe('Cefazolin2g c/8 hs iv; gentamicin 3mg/kg/day iv; rifampicin 600 mg c/12 hsvo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RjIURpjUvxLW"
   },
   "outputs": [],
   "source": [
    "# group annotations around a clinical procedure mention, based on the annotation label\n",
    "def group_annotations_strict(annotations):\n",
    "    groups = []\n",
    "    i = 0\n",
    "    while i < len(annotations):\n",
    "        if annotations[i]['entity_group'] == 'LABEL_0':\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        group = [] # for the strict strategy, a group is a B (or many Bs), followed by 1 or more Is\n",
    "        if annotations[i]['entity_group'] == 'LABEL_1':\n",
    "            group.append(annotations[i])\n",
    "            i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_1'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_2'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            groups.append(group)\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SsntbSAryqAn"
   },
   "outputs": [],
   "source": [
    "# merge grouped annotations to form a complete entity mention\n",
    "def merge_annotation_group_entries(annotation_group, sentence):\n",
    "    start = annotation_group[0]['start']\n",
    "    end = annotation_group[len(annotation_group) - 1]['end']\n",
    "    text = sentence[start:end]\n",
    "    return {'start': start, 'end': end, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8f574bI6qEwE"
   },
   "outputs": [],
   "source": [
    "def get_mentions(sentence):\n",
    "    out = classifier(sentence, **tokenizer_kwargs)\n",
    "    #print(out[0]['label'] == 'ent')\n",
    "    if out[0]['label'] == \"ent\":\n",
    "        ner_result = ner_pipe(sentence)\n",
    "        return [{'start': mention['start'], 'end': mention['end'], 'text': mention['word']} for mention in ner_result]\n",
    "        #annotation_groups = group_annotations_strict(ner_result)        \n",
    "        #return [merge_annotation_group_entries(group, sentence) for group in annotation_groups]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_mention_offset(mentions, sentence, original_token_offsets, original_sentence, filename, batch_start):\n",
    "    original_mention_offsets = []\n",
    "    current_offset_idx = 0 \n",
    "    for mention in mentions:\n",
    "        start = mention['start']\n",
    "        end = mention['end']\n",
    "        \n",
    "        original_start = -1\n",
    "        original_end = -1\n",
    "        while current_offset_idx < len(original_token_offsets):\n",
    "            token = original_token_offsets[current_offset_idx]\n",
    "            \n",
    "            if token[3] <= start:\n",
    "                original_start = token[0]\n",
    "            \n",
    "            if token[4] >= end:\n",
    "                original_end = token[1]\n",
    "                break\n",
    "            \n",
    "            current_offset_idx += 1\n",
    "\n",
    "        sentence_no_spaces = sentence[start:end].replace(' ', '')\n",
    "        original_sentence_no_spaces = original_sentence[original_start:original_end].replace(' ', '')\n",
    "        # check whether the detected span is contained in the original\n",
    "        if sentence_no_spaces != original_sentence_no_spaces and not(sentence_no_spaces in original_sentence_no_spaces):\n",
    "            print('potential offset issue ', filename, sentence[start:end], original_sentence[original_start:original_end])\n",
    "        if original_start == -1 or original_end == -1:\n",
    "            print('mention not found ', filename, mention)\n",
    "            \n",
    "        original_mention_offsets.append({\n",
    "            'filename': filename, \n",
    "            'start_span':original_start+batch_start, \n",
    "            'end_span':original_end+batch_start, \n",
    "            'text': original_sentence[original_start:original_end]\n",
    "        })\n",
    "    \n",
    "    return original_mention_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Treatment was started with piperacillin tazobactam 4.5g iv every 8h (after taking blood cultures) for 4 days.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Treatment was started with piperacillin tazobactam 4 . 5g iv every 8h ( after taking blood cultures ) for 4 days .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'FARMACO',\n",
       "  'score': 0.9969311,\n",
       "  'word': 'piperacillin tazobactam',\n",
       "  'start': 27,\n",
       "  'end': 50}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb43SpEHQsI9",
    "outputId": "40405391-c271-4057-ba9f-457c8d2f8528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 27, 'end': 50, 'text': 'piperacillin tazobactam'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = get_mentions(tokenized_sentence)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'filename',\n",
       "  'start_span': 37,\n",
       "  'end_span': 60,\n",
       "  'text': 'piperacillin tazobactam'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_original_mention_offset(mentions, tokenized_sentence, original_token_offsets, text, 'filename', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_spans(doc, mentions):\n",
    "    spans = []\n",
    "    current_offset_idx = 0 \n",
    "    for mention in mentions:\n",
    "        start = mention['start']\n",
    "        end = mention['end']\n",
    "        \n",
    "        span_start = -1\n",
    "        span_end = -1\n",
    "        while current_offset_idx < len(doc):\n",
    "            token = doc[current_offset_idx]\n",
    "            \n",
    "            if token.idx <= start:\n",
    "                span_start = token.idx\n",
    "            \n",
    "            if token.idx + len(token) >= end:\n",
    "                span_end = token.idx + len(token)\n",
    "                break\n",
    "            \n",
    "            current_offset_idx += 1\n",
    "        \n",
    "        spans.append(doc.char_span(span_start, span_end))\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "def get_filtered_mentions(doc, phrase_mentions, mentions):\n",
    "    span_mentions = get_mention_spans(doc, mentions)\n",
    "    #phrase_mentions.extend(span_mentions)\n",
    "    filtered_matches = filter_spans(span_mentions)\n",
    "    return [{'start': doc[match.start].idx,'end': doc[match.end-1].idx + len(doc[match.end-1]), 'text':doc[match.start:match.end]} for match in filtered_matches if len(match) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylvia/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 37500 sentences\n",
      "processed 38000 sentences\n",
      "processed 38500 sentences\n",
      "processed 39000 sentences\n",
      "processed 39500 sentences\n",
      "processed 40000 sentences\n",
      "processed 40500 sentences\n",
      "processed 41000 sentences\n",
      "processed 41500 sentences\n",
      "processed 42000 sentences\n",
      "processed 42500 sentences\n",
      "processed 43000 sentences\n",
      "processed 43500 sentences\n",
      "processed 44000 sentences\n",
      "processed 44500 sentences\n",
      "processed 45000 sentences\n",
      "processed 45500 sentences\n",
      "processed 46000 sentences\n",
      "processed 46500 sentences\n",
      "processed 47000 sentences\n",
      "processed 47500 sentences\n",
      "processed 48000 sentences\n",
      "processed 48500 sentences\n",
      "processed 49000 sentences\n",
      "processed 49500 sentences\n",
      "processed 50000 sentences\n",
      "processed 50500 sentences\n",
      "processed 51000 sentences\n",
      "processed 51500 sentences\n",
      "processed 52000 sentences\n",
      "processed 52500 sentences\n",
      "processed 53000 sentences\n",
      "processed 53500 sentences\n",
      "processed 54000 sentences\n",
      "processed 54500 sentences\n"
     ]
    }
   ],
   "source": [
    "original_mentions_list = []\n",
    "for index, row in df_dev_sentences[df_dev_sentences['lang']==lang].iterrows():\n",
    "    text = row['text'].rstrip()\n",
    "    tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "    #doc = nlp(tokenized_sentence)\n",
    "    #phrase_mentions = get_phrase_mentions(doc)\n",
    "    filtered_mentions = get_mentions(tokenized_sentence)\n",
    "    \n",
    "    #filtered_mentions = get_filtered_mentions(doc, phrase_mentions, mentions)\n",
    "    \n",
    "    original_mentions = get_original_mention_offset(filtered_mentions, tokenized_sentence, original_token_offsets, text, row['filename'], row['batch_start'])\n",
    "    original_mentions_list.extend(original_mentions)\n",
    "    \n",
    "    if (index+1) % 500 == 0:\n",
    "        print(f'processed {index+1} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>start_span</th>\n",
       "      <th>end_span</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>2557</td>\n",
       "      <td>2569</td>\n",
       "      <td>Cefazolina2g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>2581</td>\n",
       "      <td>2592</td>\n",
       "      <td>gentamicina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>2608</td>\n",
       "      <td>2619</td>\n",
       "      <td>rifampicina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>3265</td>\n",
       "      <td>3274</td>\n",
       "      <td>amikacina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>3280</td>\n",
       "      <td>3291</td>\n",
       "      <td>vancomicina</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  start_span  end_span          text\n",
       "0  casos_clinicos_cardiologia10        2557      2569  Cefazolina2g\n",
       "1  casos_clinicos_cardiologia10        2581      2592   gentamicina\n",
       "2  casos_clinicos_cardiologia10        2608      2619   rifampicina\n",
       "3  casos_clinicos_cardiologia10        3265      3274     amikacina\n",
       "4  casos_clinicos_cardiologia10        3280      3291   vancomicina"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mentions = pd.DataFrame.from_records(original_mentions_list)\n",
    "df_mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions['label'] = target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions[['filename', 'label', 'start_span', 'end_span', 'text']].to_csv(result_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# es - |0.8636|0.9028|0.8827\n",
    "# en - |0.8507|0.8924|0.8711\n",
    "# it - |0.8606|0.8789|0.8697"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
