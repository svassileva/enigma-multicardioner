{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TRv18xWQbOhr"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "task = \"ner\"\n",
    "base_model = 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es'\n",
    "model_name = \"roberta-base-biomedical-clinical-es\"\n",
    "output_path = 'output/models'\n",
    "model_checkpoint = f\"{output_path}/{model_name}-finetuned-{task}\"\n",
    "num_labels = 3\n",
    "target_label = 'ENFERMEDAD'\n",
    "\n",
    "sentences_file = 'custom_dataset_track1/dev_sentences.tsv'\n",
    "result_file = 'multicardioner_track1_cardioccc_dev_predictions.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>batch_number</th>\n",
       "      <th>batch_start</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Anamnesis\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>Sexo masculino, 79 años.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>Autoválido.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>Procedente de Salto.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>Antecedentes patológicos: –Hipertensión arteri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  batch_number  batch_start  \\\n",
       "0  casos_clinicos_cardiologia10             1            0   \n",
       "1  casos_clinicos_cardiologia10             2           10   \n",
       "2  casos_clinicos_cardiologia10             3           35   \n",
       "3  casos_clinicos_cardiologia10             4           47   \n",
       "4  casos_clinicos_cardiologia10             5           68   \n",
       "\n",
       "                                                text  \n",
       "0                                        Anamnesis\\n  \n",
       "1                         Sexo masculino, 79 años.\\n  \n",
       "2                                      Autoválido.\\n  \n",
       "3                             Procedente de Salto.\\n  \n",
       "4  Antecedentes patológicos: –Hipertensión arteri...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_sentences = pd.read_csv(sentences_file, sep='\\t')\n",
    "df_dev_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKENIZATION_REGEX = re.compile(r'([0-9\\w]+|[^0-9\\w])', re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    original_token_offsets = []\n",
    "\n",
    "    offset = 0\n",
    "    new_offset = 0\n",
    "    nonspace_token_seen = False\n",
    "\n",
    "    tokens = [t for t in TOKENIZATION_REGEX.split(text) if t]\n",
    "    for t in tokens:\n",
    "        if not t.isspace():\n",
    "            original_token_offsets.append([offset, offset + len(t), t, new_offset, new_offset + len(t)])\n",
    "            nonspace_token_seen = True\n",
    "            new_offset += len(t) + 1\n",
    "        offset += len(t)\n",
    "        \n",
    "\n",
    "    tokenized_sentence = ' '.join([l[2] for l in original_token_offsets])\n",
    "\n",
    "    # store original token offsets\n",
    "    # pass the tokenized string for prediction\n",
    "    return tokenized_sentence, original_token_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RXA3_zObUkl",
    "outputId": "41533b0a-3871-4ceb-ee93-4f4e9fe6350c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline, TokenClassificationPipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3Hp7VwIeuFD8"
   },
   "outputs": [],
   "source": [
    "ner_pipe = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", stride=0, pipeline_class=TokenClassificationPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RjIURpjUvxLW"
   },
   "outputs": [],
   "source": [
    "# group annotations around a clinical procedure mention, based on the annotation label\n",
    "def group_annotations_strict(annotations):\n",
    "    groups = []\n",
    "    i = 0\n",
    "    while i < len(annotations):\n",
    "        if annotations[i]['entity_group'] == 'LABEL_0':\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        group = [] # for the strict strategy, a group is a B (or many Bs), followed by 1 or more Is\n",
    "        if annotations[i]['entity_group'] == 'LABEL_1':\n",
    "            group.append(annotations[i])\n",
    "            i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_1'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_2'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            groups.append(group)\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SsntbSAryqAn"
   },
   "outputs": [],
   "source": [
    "# merge grouped annotations to form a complete entity mention\n",
    "def merge_annotation_group_entries(annotation_group, sentence):\n",
    "    start = annotation_group[0]['start']\n",
    "    end = annotation_group[len(annotation_group) - 1]['end']\n",
    "    text = sentence[start:end]\n",
    "    return {'start': start, 'end': end, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8f574bI6qEwE"
   },
   "outputs": [],
   "source": [
    "def get_mentions(sentence):\n",
    "    annotation_groups = group_annotations_strict(ner_pipe(sentence))\n",
    "    return [merge_annotation_group_entries(group, sentence) for group in annotation_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_mention_offset(mentions, sentence, original_token_offsets, original_sentence, filename, batch_start):\n",
    "    original_mention_offsets = []\n",
    "    current_offset_idx = 0 \n",
    "    for mention in mentions:\n",
    "        start = mention['start']\n",
    "        end = mention['end']\n",
    "        \n",
    "        original_start = -1\n",
    "        original_end = -1\n",
    "        while current_offset_idx < len(original_token_offsets):\n",
    "            token = original_token_offsets[current_offset_idx]\n",
    "            \n",
    "            if token[3] <= start:\n",
    "                original_start = token[0]\n",
    "            \n",
    "            if token[4] >= end:\n",
    "                original_end = token[1]\n",
    "                break\n",
    "            \n",
    "            current_offset_idx += 1\n",
    "\n",
    "        sentence_no_spaces = sentence[start:end].replace(' ', '')\n",
    "        original_sentence_no_spaces = original_sentence[original_start:original_end].replace(' ', '')\n",
    "        # check whether the detected span is contained in the original\n",
    "        if sentence_no_spaces != original_sentence_no_spaces and not(sentence_no_spaces in original_sentence_no_spaces):\n",
    "            print('potential offset issue ', filename, sentence[start:end], original_sentence[original_start:original_end])\n",
    "        if original_start == -1 or original_end == -1:\n",
    "            print('mention not found ', filename, mention)\n",
    "            \n",
    "        original_mention_offsets.append({\n",
    "            'filename': filename, \n",
    "            'start_span':original_start+batch_start, \n",
    "            'end_span':original_end+batch_start, \n",
    "            'text': original_sentence[original_start:original_end]\n",
    "        })\n",
    "    \n",
    "    return original_mention_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"• ECG: bloqueo aurículo-ventricular (BAV) de primer grado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'• ECG : bloqueo aurículo - ventricular ( BAV ) de primer grado'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LABEL_0',\n",
       "  'score': 0.9996733,\n",
       "  'word': ' • ECG :',\n",
       "  'start': 0,\n",
       "  'end': 7},\n",
       " {'entity_group': 'LABEL_1',\n",
       "  'score': 0.99480635,\n",
       "  'word': ' bloqueo',\n",
       "  'start': 8,\n",
       "  'end': 15},\n",
       " {'entity_group': 'LABEL_2',\n",
       "  'score': 0.9984991,\n",
       "  'word': ' aurículo - ventricular',\n",
       "  'start': 16,\n",
       "  'end': 38},\n",
       " {'entity_group': 'LABEL_0',\n",
       "  'score': 0.8087198,\n",
       "  'word': ' (',\n",
       "  'start': 39,\n",
       "  'end': 40},\n",
       " {'entity_group': 'LABEL_1',\n",
       "  'score': 0.96049446,\n",
       "  'word': ' BAV',\n",
       "  'start': 41,\n",
       "  'end': 44},\n",
       " {'entity_group': 'LABEL_2',\n",
       "  'score': 0.99676716,\n",
       "  'word': ' ) de primer grado',\n",
       "  'start': 45,\n",
       "  'end': 62}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb43SpEHQsI9",
    "outputId": "40405391-c271-4057-ba9f-457c8d2f8528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 8, 'end': 38, 'text': 'bloqueo aurículo - ventricular'},\n",
       " {'start': 41, 'end': 62, 'text': 'BAV ) de primer grado'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = get_mentions(tokenized_sentence)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'filename',\n",
       "  'start_span': 17,\n",
       "  'end_span': 45,\n",
       "  'text': 'bloqueo aurículo-ventricular'},\n",
       " {'filename': 'filename',\n",
       "  'start_span': 47,\n",
       "  'end_span': 67,\n",
       "  'text': 'BAV) de primer grado'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_original_mention_offset(mentions, tokenized_sentence, original_token_offsets, text, 'filename', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 500 sentences\n",
      "processed 1000 sentences\n",
      "processed 1500 sentences\n",
      "processed 2000 sentences\n",
      "processed 2500 sentences\n",
      "processed 3000 sentences\n",
      "processed 3500 sentences\n",
      "processed 4000 sentences\n",
      "processed 4500 sentences\n",
      "processed 5000 sentences\n",
      "processed 5500 sentences\n",
      "processed 6000 sentences\n",
      "processed 6500 sentences\n",
      "processed 7000 sentences\n",
      "processed 7500 sentences\n",
      "processed 8000 sentences\n",
      "processed 8500 sentences\n",
      "processed 9000 sentences\n",
      "processed 9500 sentences\n",
      "processed 10000 sentences\n",
      "processed 10500 sentences\n",
      "processed 11000 sentences\n",
      "processed 11500 sentences\n",
      "processed 12000 sentences\n",
      "processed 12500 sentences\n",
      "processed 13000 sentences\n",
      "processed 13500 sentences\n",
      "processed 14000 sentences\n",
      "processed 14500 sentences\n",
      "processed 15000 sentences\n",
      "processed 15500 sentences\n",
      "processed 16000 sentences\n",
      "processed 16500 sentences\n",
      "processed 17000 sentences\n",
      "processed 17500 sentences\n",
      "processed 18000 sentences\n",
      "processed 18500 sentences\n",
      "processed 19000 sentences\n"
     ]
    }
   ],
   "source": [
    "original_mentions_list =[]\n",
    "for index, row in df_dev_sentences.iterrows():\n",
    "    text = row['text'].rstrip()\n",
    "    tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "    mentions = get_mentions(tokenized_sentence)\n",
    "    original_mentions = get_original_mention_offset(mentions, tokenized_sentence, original_token_offsets, text, row['filename'], row['batch_start'])\n",
    "    original_mentions_list.extend(original_mentions)\n",
    "    \n",
    "    if (index+1) % 500 == 0:\n",
    "        print(f'processed {index+1} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>start_span</th>\n",
       "      <th>end_span</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>95</td>\n",
       "      <td>124</td>\n",
       "      <td>Hipertensión arterial crónica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>126</td>\n",
       "      <td>139</td>\n",
       "      <td>Ex-tabaquista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>142</td>\n",
       "      <td>215</td>\n",
       "      <td>Diabetes  mellitus  tipo  2  con  repercusione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>217</td>\n",
       "      <td>238</td>\n",
       "      <td>cardiopatía isquémica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>240</td>\n",
       "      <td>260</td>\n",
       "      <td>arteriopatía de MMII</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  start_span  end_span  \\\n",
       "0  casos_clinicos_cardiologia10          95       124   \n",
       "1  casos_clinicos_cardiologia10         126       139   \n",
       "2  casos_clinicos_cardiologia10         142       215   \n",
       "3  casos_clinicos_cardiologia10         217       238   \n",
       "4  casos_clinicos_cardiologia10         240       260   \n",
       "\n",
       "                                                text  \n",
       "0                      Hipertensión arterial crónica  \n",
       "1                                      Ex-tabaquista  \n",
       "2  Diabetes  mellitus  tipo  2  con  repercusione...  \n",
       "3                              cardiopatía isquémica  \n",
       "4                               arteriopatía de MMII  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mentions = pd.DataFrame.from_records(original_mentions_list)\n",
    "df_mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions['label'] = target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions[['filename', 'label', 'start_span', 'end_span', 'text']].to_csv(result_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
