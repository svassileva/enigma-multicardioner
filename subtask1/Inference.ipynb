{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TRv18xWQbOhr"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "task = \"ner\"\n",
    "base_model = 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es'\n",
    "model_name = \"roberta-base-biomedical-clinical-es\"\n",
    "output_path = 'output/models'\n",
    "model_checkpoint = f\"{output_path}/{model_name}-finetuned-{task}\"\n",
    "num_labels = 3\n",
    "target_label = 'ENFERMEDAD'\n",
    "\n",
    "sentences_file = 'custom_dataset_track1/test_sentences.tsv'\n",
    "result_file = 'output/multicardioner_track1_cardioccc_dev_predictions.tsv'\n",
    "spacy_phrases_file = 'data/diseases.pickle'\n",
    "lang = 'es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>batch_number</th>\n",
       "      <th>batch_start</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Àmbit: atenció primària (AP).\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>Motiu de consulta: dona de 26 anys que consult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>3</td>\n",
       "      <td>114</td>\n",
       "      <td>Havia estat visitada per dolor abdominal al ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>4</td>\n",
       "      <td>183</td>\n",
       "      <td>Explica dolor abdominal de 2 setmanes d’evoluc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multicardioner_test+bg_1</td>\n",
       "      <td>5</td>\n",
       "      <td>303</td>\n",
       "      <td>Història clínica: antecedents personals sense ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   filename  batch_number  batch_start  \\\n",
       "0  multicardioner_test+bg_1             1            0   \n",
       "1  multicardioner_test+bg_1             2           31   \n",
       "2  multicardioner_test+bg_1             3          114   \n",
       "3  multicardioner_test+bg_1             4          183   \n",
       "4  multicardioner_test+bg_1             5          303   \n",
       "\n",
       "                                                text  \n",
       "0                    Àmbit: atenció primària (AP).\\n  \n",
       "1  Motiu de consulta: dona de 26 anys que consult...  \n",
       "2  Havia estat visitada per dolor abdominal al ce...  \n",
       "3  Explica dolor abdominal de 2 setmanes d’evoluc...  \n",
       "4  Història clínica: antecedents personals sense ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_sentences = pd.read_csv(sentences_file, sep='\\t', keep_default_na=False)\n",
    "df_dev_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKENIZATION_REGEX = re.compile(r\"([0-9\\w]+|[^0-9\\w])\", re.UNICODE) # ’ - es, ' - en, ' - it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    original_token_offsets = []\n",
    "\n",
    "    offset = 0\n",
    "    new_offset = 0\n",
    "    nonspace_token_seen = False\n",
    "\n",
    "    tokens = [t for t in TOKENIZATION_REGEX.split(text) if t]\n",
    "    for t in tokens:\n",
    "        if not t.isspace():\n",
    "            original_token_offsets.append([offset, offset + len(t), t, new_offset, new_offset + len(t)])\n",
    "            nonspace_token_seen = True\n",
    "            new_offset += len(t) + 1\n",
    "        offset += len(t)\n",
    "        \n",
    "\n",
    "    tokenized_sentence = ' '.join([l[2] for l in original_token_offsets])\n",
    "\n",
    "    # store original token offsets\n",
    "    # pass the tokenized string for prediction\n",
    "    return tokenized_sentence, original_token_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RXA3_zObUkl",
    "outputId": "41533b0a-3871-4ceb-ee93-4f4e9fe6350c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline, TokenClassificationPipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3Hp7VwIeuFD8"
   },
   "outputs": [],
   "source": [
    "ner_pipe = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", stride=0, pipeline_class=TokenClassificationPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RjIURpjUvxLW"
   },
   "outputs": [],
   "source": [
    "# group annotations around a clinical procedure mention, based on the annotation label\n",
    "def group_annotations_strict(annotations):\n",
    "    groups = []\n",
    "    i = 0\n",
    "    while i < len(annotations):\n",
    "        if annotations[i]['entity_group'] == 'LABEL_0':\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        group = [] # for the strict strategy, a group is a B (or many Bs), followed by 1 or more Is\n",
    "        if annotations[i]['entity_group'] == 'LABEL_1':\n",
    "            group.append(annotations[i])\n",
    "            i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_1'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            while (i < len(annotations) and annotations[i]['entity_group'] == 'LABEL_2'):\n",
    "                group.append(annotations[i])\n",
    "                i += 1\n",
    "\n",
    "            groups.append(group)\n",
    "        else:\n",
    "            i+=1\n",
    "            continue\n",
    "\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SsntbSAryqAn"
   },
   "outputs": [],
   "source": [
    "# merge grouped annotations to form a complete entity mention\n",
    "def merge_annotation_group_entries(annotation_group, sentence):\n",
    "    start = annotation_group[0]['start']\n",
    "    end = annotation_group[len(annotation_group) - 1]['end']\n",
    "    text = sentence[start:end]\n",
    "    return {'start': start, 'end': end, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8f574bI6qEwE"
   },
   "outputs": [],
   "source": [
    "def get_mentions(sentence):\n",
    "    annotation_groups = group_annotations_strict(ner_pipe(sentence))\n",
    "    return [merge_annotation_group_entries(group, sentence) for group in annotation_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_mention_offset(mentions, sentence, original_token_offsets, original_sentence, filename, batch_start):\n",
    "    original_mention_offsets = []\n",
    "    current_offset_idx = 0 \n",
    "    for mention in mentions:\n",
    "        start = mention['start']\n",
    "        end = mention['end']\n",
    "        \n",
    "        original_start = -1\n",
    "        original_end = -1\n",
    "        while current_offset_idx < len(original_token_offsets):\n",
    "            token = original_token_offsets[current_offset_idx]\n",
    "            \n",
    "            if token[3] <= start:\n",
    "                original_start = token[0]\n",
    "            \n",
    "            if token[4] >= end:\n",
    "                original_end = token[1]\n",
    "                break\n",
    "            \n",
    "            current_offset_idx += 1\n",
    "\n",
    "        sentence_no_spaces = sentence[start:end].replace(' ', '')\n",
    "        original_sentence_no_spaces = original_sentence[original_start:original_end].replace(' ', '')\n",
    "        # check whether the detected span is contained in the original\n",
    "        if sentence_no_spaces != original_sentence_no_spaces and not(sentence_no_spaces in original_sentence_no_spaces):\n",
    "            print('potential offset issue ', filename, sentence[start:end], original_sentence[original_start:original_end])\n",
    "        if original_start == -1 or original_end == -1:\n",
    "            print('mention not found ', filename, mention)\n",
    "            \n",
    "        original_mention_offsets.append({\n",
    "            'filename': filename, \n",
    "            'start_span':original_start+batch_start, \n",
    "            'end_span':original_end+batch_start, \n",
    "            'text': original_sentence[original_start:original_end]\n",
    "        })\n",
    "    \n",
    "    return original_mention_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Explica dolor abdominal de 2 setmanes d’evolució i basques sense vòmits, es programa visita presencial per a l’endemà.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Explica dolor abdominal de 2 setmanes d’evolució i basques sense vòmits , es programa visita presencial per a l’endemà .'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LABEL_0',\n",
       "  'score': 0.9998,\n",
       "  'word': ' Explica dolor abdominal de 2 setmanes d’evolució i basques sense vòmits , es programa visita presencial per a l’endemà .',\n",
       "  'start': 0,\n",
       "  'end': 120}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipe(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lb43SpEHQsI9",
    "outputId": "40405391-c271-4057-ba9f-457c8d2f8528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = get_mentions(tokenized_sentence)\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_original_mention_offset(mentions, tokenized_sentence, original_token_offsets, text, 'filename', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import pickle\n",
    "\n",
    "with open(spacy_phrases_file, 'rb') as handle:\n",
    "    matcher = pickle.load(handle)\n",
    "    \n",
    "nlp = spacy.blank(lang, vocab=matcher.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_mentions(doc):\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc, as_spans=True)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dolor abdominal]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(tokenized_sentence)\n",
    "get_phrase_mentions(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_spans(doc, mentions):\n",
    "    spans = []\n",
    "    current_offset_idx = 0 \n",
    "    for mention in mentions:\n",
    "        start = mention['start']\n",
    "        end = mention['end']\n",
    "        \n",
    "        span_start = -1\n",
    "        span_end = -1\n",
    "        while current_offset_idx < len(doc):\n",
    "            token = doc[current_offset_idx]\n",
    "            \n",
    "            if token.idx <= start:\n",
    "                span_start = token.idx\n",
    "            \n",
    "            if token.idx + len(token) >= end:\n",
    "                span_end = token.idx + len(token)\n",
    "                break\n",
    "            \n",
    "            current_offset_idx += 1\n",
    "        \n",
    "        spans.append(doc.char_span(span_start, span_end))\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "def get_filtered_mentions(doc, phrase_mentions, mentions):\n",
    "    span_mentions = get_mention_spans(doc, mentions)\n",
    "    phrase_mentions.extend(span_mentions)\n",
    "    filtered_matches = filter_spans(span_mentions)\n",
    "    return [{'start': doc[match.start].idx,'end': doc[match.end-1].idx + len(doc[match.end-1]), 'text':doc[match.start:match.end]} for match in filtered_matches if len(match) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 500 sentences\n",
      "processed 1000 sentences\n",
      "processed 1500 sentences\n",
      "processed 2000 sentences\n",
      "processed 2500 sentences\n",
      "processed 3000 sentences\n",
      "processed 3500 sentences\n",
      "processed 4000 sentences\n",
      "processed 4500 sentences\n",
      "processed 5000 sentences\n",
      "processed 5500 sentences\n",
      "processed 6000 sentences\n",
      "processed 6500 sentences\n",
      "processed 7000 sentences\n",
      "processed 7500 sentences\n",
      "processed 8000 sentences\n",
      "processed 8500 sentences\n",
      "processed 9000 sentences\n",
      "processed 9500 sentences\n",
      "processed 10000 sentences\n",
      "processed 10500 sentences\n",
      "processed 11000 sentences\n",
      "processed 11500 sentences\n",
      "processed 12000 sentences\n",
      "processed 12500 sentences\n",
      "processed 13000 sentences\n",
      "processed 13500 sentences\n",
      "processed 14000 sentences\n",
      "processed 14500 sentences\n",
      "processed 15000 sentences\n",
      "processed 15500 sentences\n",
      "processed 16000 sentences\n",
      "processed 16500 sentences\n",
      "processed 17000 sentences\n",
      "processed 17500 sentences\n",
      "processed 18000 sentences\n",
      "processed 18500 sentences\n",
      "processed 19000 sentences\n"
     ]
    }
   ],
   "source": [
    "original_mentions_list = []\n",
    "for index, row in df_dev_sentences.iterrows():\n",
    "    text = row['text'].rstrip()\n",
    "    tokenized_sentence, original_token_offsets = tokenize(text)\n",
    "    doc = nlp(tokenized_sentence)\n",
    "    phrase_mentions = get_phrase_mentions(doc)\n",
    "    mentions = get_mentions(tokenized_sentence)\n",
    "    \n",
    "    filtered_mentions = get_filtered_mentions(doc, phrase_mentions, mentions)\n",
    "    \n",
    "    original_mentions = get_original_mention_offset(filtered_mentions, tokenized_sentence, original_token_offsets, text, row['filename'], row['batch_start'])\n",
    "    original_mentions_list.extend(original_mentions)\n",
    "    \n",
    "    if (index+1) % 500 == 0:\n",
    "        print(f'processed {index+1} sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>start_span</th>\n",
       "      <th>end_span</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>95</td>\n",
       "      <td>124</td>\n",
       "      <td>Hipertensión arterial crónica</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>126</td>\n",
       "      <td>139</td>\n",
       "      <td>Ex-tabaquista</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>142</td>\n",
       "      <td>215</td>\n",
       "      <td>Diabetes  mellitus  tipo  2  con  repercusione...</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>217</td>\n",
       "      <td>238</td>\n",
       "      <td>cardiopatía isquémica</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>casos_clinicos_cardiologia10</td>\n",
       "      <td>240</td>\n",
       "      <td>260</td>\n",
       "      <td>arteriopatía de MMII</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename  start_span  end_span  \\\n",
       "0  casos_clinicos_cardiologia10          95       124   \n",
       "1  casos_clinicos_cardiologia10         126       139   \n",
       "2  casos_clinicos_cardiologia10         142       215   \n",
       "3  casos_clinicos_cardiologia10         217       238   \n",
       "4  casos_clinicos_cardiologia10         240       260   \n",
       "\n",
       "                                                text       label  \n",
       "0                      Hipertensión arterial crónica  ENFERMEDAD  \n",
       "1                                      Ex-tabaquista  ENFERMEDAD  \n",
       "2  Diabetes  mellitus  tipo  2  con  repercusione...  ENFERMEDAD  \n",
       "3                              cardiopatía isquémica  ENFERMEDAD  \n",
       "4                               arteriopatía de MMII  ENFERMEDAD  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mentions = pd.DataFrame.from_records(original_mentions_list)\n",
    "df_mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions['label'] = target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mentions[['filename', 'label', 'start_span', 'end_span', 'text']].to_csv(result_file, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
